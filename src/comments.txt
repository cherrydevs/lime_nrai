let mut X = x.clone();
// insert the 1 values as the first column, so that each row becomes [1, x]
        X.insert_column(0, 1.0);
// insert another column with x^2 as the last, so that each row becomes [1, x, x^2]
        X.insert_column_with(2, x.column_iter(0).map(|x| x * x));
        println!("{:?}", &X);

// now we compute the weights that give the lowest error for y - (X * w)
// by w = inv(X^T * X) * (X^T * y)
// Note the use of referencing X, w, and y so we don't move them into
// a computation.
// Because we're doing linear regression and creating the matrix we take the inverse
// of in a particular way we don't check if the inverse exists here, but in general
// for arbitary matrices you cannot assume that an inverse exists.
        let w = (X.transpose() * &X).inverse().unwrap() * (X.transpose() * &y);
// now predict y using the learned weights
        let predictions = &X * &w;
// compute the error for each y and predicted y
        let errors = &y - &predictions;
// multiply each error by itself to get the squared error
// and sum into a unit matrix by taking the inner prouct
// then divide by the number of rows to get mean squared error
        let mean_squared_error = (errors.transpose() * &errors).get(0, 0) / x.rows() as f32;

        println!("MSE: {}", mean_squared_error);
        //assert!(mean_squared_error > 0.41);
        //assert!(mean_squared_error < 0.42);
        println!("Predicted y values:\n{:?}", &predictions);
        println!("Actual y values:\n{:?}", &y);

        // now we have a model we can predict outside the range we trained the weights on
        let test_x: Matrix<f32> = Matrix::column(vec![-3.0, -1.0, 0.5, 2.5, 13.0, 14.0]);
        let test_y = test_x.map(|x| x.powi(2) + x.sin());
        let mut test_X = test_x.clone();
        test_X.insert_column(0, 1.0);
        test_X.insert_column_with(2, test_x.column_iter(0).map(|x| x * x));

        // unsurprisingly the model has generalised quite well but
        // did better on the training data
        println!("Unseen x values:\n{:?}", test_x);
        println!("Unseen y predictions:\n{:?}", &test_X * &w);
        println!("Unseen y actual values:\n{:?}", test_y);
        let errors = &test_y - (&test_X * &w);
        let mean_squared_error = (errors.transpose() * &errors).get(0, 0) / test_x.rows() as f32;
        println!("MSE on unseen values: {}", mean_squared_error);
        // assert!(mean_squared_error < 1.0);
        // assert!(mean_squared_error > 0.99);